from __future__ import annotations

import gzip
import shutil
import tempfile
import warnings
from collections import defaultdict
from collections.abc import Generator
from pathlib import Path
from typing import Annotated, Literal, Optional

import func_timeout
import numpy as np
import polars as pl
import pyarrow as pa
from pyarrow import csv as pcsv, parquet as pq
from pydantic import (
    AliasChoices,
    BaseModel,
    Field,
    computed_field,
    field_validator,
)

from .schemas import ReportSchema
from .types import ExistentPath, Mb2Bytes
from .universal_batches import UniversalBatch
from .utils import ReportBar


class ArrowParquetReader:
    def __init__(
        self,
        file: str | Path,
        use_cols: list = None,
        use_threads: bool = True,
        **kwargs,
    ):
        self.file = Path(file).expanduser().absolute()
        if not self.file.exists():
            raise FileNotFoundError()

        self.reader = pq.ParquetFile(file)
        self.__current_group = 0

        self.__use_cols = use_cols
        self.__use_threads = use_threads

    def __len__(self):
        return self.reader.num_row_groups

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.reader.close()

    def __iter__(self):
        return self

    def _mutate_next(self, batch: pa.RecordBatch):
        return batch

    def __next__(self):
        old_group = self.__current_group
        # Check if it is the last row group
        if old_group < self.reader.num_row_groups:
            self.__current_group += 1

            batch = self.reader.read_row_group(
                old_group, columns=self.__use_cols, use_threads=self.__use_threads
            )

            mutated = self._mutate_next(batch)
            return mutated
        raise StopIteration()

    @property
    def batch_size(self):
        return int(self.file.stat().st_size / self.reader.num_row_groups)


class BinomReader(ArrowParquetReader):
    """
    Class for reading methylation data from .parquet files generated by
    :class:`BinomialData`.

    Parameters
    ----------
    file
        Path to the .parquet file.
    methylation_pvalue
        Pvalue with which cytosine will be considered methylated.
    """

    def __init__(self, file: str | Path, methylation_pvalue=0.05, **kwargs):
        super().__init__(file, **kwargs)

        if 0 < methylation_pvalue <= 1:
            self.methylation_pvalue = methylation_pvalue
        else:
            self.methylation_pvalue = 0.05
            warnings.warn(
                f"P-value needs to be in (0;1] interval, not {methylation_pvalue}. "
                f"Setting to default ({self.methylation_pvalue})",
                stacklevel=1,
            )

    def _mutate_next(self, batch: pa.RecordBatch):
        df = pl.from_arrow(batch)

        mutated = df.with_columns(
            (pl.col("p_value") <= self.methylation_pvalue)
            .cast(pl.UInt8)
            .alias("count_m")
        ).with_columns(
            [
                pl.col("context").alias("trinuc"),
                pl.lit(1).alias("count_total"),
                pl.col("count_m").cast(pl.Float64).alias("density"),
            ]
        )

        return UniversalBatch(mutated, batch)


class ArrowReaderCSV2(BaseModel):
    file: ExistentPath
    block_size_mb: Mb2Bytes
    report_schema: ReportSchema
    memory_pool: type[pa.MemoryPool] = Field(default_factory=pa.system_memory_pool)
    kwargs: dict = Field(default_factory=dict)
    use_threads: bool = Field(default=True)

    @computed_field
    def convert_options(self):
        return pcsv.ConvertOptions(
            column_types=self.report_schema.value.arrow, strings_can_be_null=False
        )

    @computed_field
    def parse_options(self):
        return pcsv.ParseOptions(
            delimiter="\t",
            quote_char=False,
            escape_char=False,
            ignore_empty_lines=True,
            invalid_row_handler=lambda _: "skip",  # TODO maybe add logging
        )

    @computed_field
    def read_options(self):
        return pcsv.ReadOptions(
            use_threads=self.use_threads,
            block_size=self.block_size_mb,
            skip_rows=0,
            skip_rows_after_names=0,
            column_names=self.report_schema.value.arrow.names,
        )

    @func_timeout.func_set_timeout(20)
    def init_arrow(self) -> pcsv.CSVStreamingReader:
        try:
            reader = pcsv.open_csv(
                self.file,
                self.read_options,
                self.parse_options,
                self.convert_options,
                self.memory_pool,
            )
            return reader
        except pa.ArrowInvalid:
            print(f"Error openning file: {self.file}")

    def __iter__(self) -> Generator[UniversalBatch]:
        reader = self.init_arrow()
        while True:
            try:
                raw = reader.read_next_batch()
                yield UniversalBatch.from_arrow(raw, self.report_schema, **self.kwargs)

            except pa.ArrowInvalid:
                # Todo add logging
                continue
            except StopIteration:
                break
        reader.close()


class UniversalReader(BaseModel):
    # Fixme
    """
    Class for batched reading methylation reports.

    Examples
    --------
    >>> reader = UniversalReader(
    ...     file="path/to/file.txt",
    ...     report_type="bismark",
    ...     use_threads=True,
    ... )
    >>> for batch in reader:
    >>>     do_something(batch)
    """

    file: ExistentPath = Field(
        title="Methylation file", description="Path to the methylation report file"
    )
    report_schema: ReportSchema = Field(
        validation_alias=AliasChoices("report_schema", "report_type"),
        title="Schema of methylation report",
        description="Either an instance of Enum :class:`ReportSchema` or one of the "
        'possible types: "bismark", "cgmap", "bedgraph", "coverage", '
        '"binom".',
    )
    use_threads: bool = Field(default=True, description="Will reading be multithreaded")
    bar_enabled: Optional[bool] = Field(
        default=False,
        validation_alias=AliasChoices("bar_enabled", "bar"),
        description="Indicate the progres bar while reading.",
    )
    allocator: Optional[Literal["system", "default", "mimalloc", "jemalloc"]] = Field(
        default="system"
    )
    cytosine_file: Optional[ExistentPath] = Field(
        default=None,
        title="Path to preprocessed cytosine file",
        description="Instance of :class:`Sequence` for reading bedGraph "
        "or .coverage reports.",
    )
    methylation_pvalue: Optional[Annotated[float, Field(gt=0, lt=1)]] = Field(
        default=None,
        title="Methylation PValue",
        description="Pvalue with which cytosine will be considered methylated.",
    )
    block_size_mb: Optional[Annotated[int, Field(gt=0)]] = Field(
        default=100,
        title="Size of batch in mb",
        description="Size of batch in bytes, which will be read from report file "
        '(for report typesother than "binom").',
    )

    @property
    def reader_kwargs(self):
        return dict(
            cytosine_file=self.cytosine_file,
            methylation_pvalue=self.methylation_pvalue,
            block_size_mb=self.block_size_mb,
        )

    @field_validator("report_schema", mode="before")
    @classmethod
    def _check_report_schema(cls, value):
        if isinstance(value, str):
            return ReportSchema(value.upper())
        elif isinstance(value, ReportSchema):
            return value
        else:
            return ValueError

    @property
    def memory_pool(self) -> type[pa.MemoryPool]:
        if self.allocator == "system":
            return pa.system_memory_pool()
        if self.allocator == "default":
            return pa.default_memory_pool()
        if self.allocator == "mimalloc":
            return pa.mimalloc_memory_pool()
        if self.allocator == "jemalloc":
            return pa.jemalloc_memory_pool()

    @property
    def _bar(self) -> ReportBar | None:
        if self.bar_enabled:
            return ReportBar(max=self.file_size)
        else:
            return None

    @property
    def file_size(self):
        """

        Returns
        -------
        int
            File size in bytes.
        """
        return self.file.stat().st_size

    def init_reader(self, infile: Path):
        if self.report_schema in {
            ReportSchema.BISMARK,
            ReportSchema.COVERAGE,
            ReportSchema.CGMAP,
            ReportSchema.BEDGRAPH,
        }:
            return ArrowReaderCSV2(
                file=infile,
                report_schema=self.report_schema,
                memory_pool=self.memory_pool,
                use_threads=self.use_threads,
                **self.reader_kwargs,
            )
        if self.report_schema == ReportSchema.BINOM:
            return BinomReader(
                infile,
                use_threads=self.use_threads,
                memory_pool=self.memory_pool,
                **self.reader_kwargs,
            )

    def __iter__(self):
        if ".gz" in self.file.suffixes:
            with (
                tempfile.NamedTemporaryFile(
                    dir=self.file.parent, prefix=self.file.stem, delete=False
                ) as infile,
                gzip.open(self.file, mode="rb") as zip_file,
            ):
                print(f"Temporarily unpack {self.file} to {infile.name}")
                shutil.copyfileobj(zip_file, infile)
                infile = Path(infile.name)
        else:
            infile = self.file

        if self._bar is not None:
            self._bar.finish()

        for batch in self.init_reader(infile):
            if self._bar is not None:
                self._bar.next()
            yield batch

        if self._bar is not None:
            self._bar.goto(self._bar.max)
            self._bar.finish()
        self.memory_pool.release_unused()
        if ".gz" in self.file.suffixes:
            infile.unlink()


class UniversalReplicatesReader:
    """
    Class for reading from replicates methylation reports. The reader sums up the
    methylation counts.

    Parameters
    ----------
    readers
        List of initialized instances of :class:`UniversalReader`.

    Examples
    --------
    >>> reader1 = UniversalReader(
    ...     file="path/to/file1.txt",
    ...     report_type="bismark",
    ...     use_threads=True,
    ... )
    >>> reader2 = UniversalReader(
    ...     file="path/to/file2.txt",
    ...     report_type="bismark",
    ...     use_threads=True,
    ... )
    >>>
    >>> for batch in UniversalReplicatesReader([reader1, reader2]):
    >>>     do_something(batch)
    """

    def __init__(
        self,
        readers: list[UniversalReader],
    ):
        self.readers = readers
        self.haste_limit = 1e9
        self.bar = None

        if any(
            map(
                lambda reader: reader.report_type.name.lower() in ["bedgraph"],
                self.readers,
            )
        ):
            warnings.warn(
                "Merging bedGraph may lead to incorrect results. Please, "
                "use other report types.",
                stacklevel=1,
            )

    def __iter__(self):
        self.bar = ReportBar(max=self.full_size)
        self.bar.start()

        self._seen_chroms = []
        self._unfinished = None

        self._readers_data = {
            idx: dict(
                iterator=iter(reader),
                read_rows=0,
                haste=0,
                finished=False,
                name=reader.file,
                chr=None,
                pos=None,
            )
            for reader, idx in zip(self.readers, range(len(self.readers)))
        }

        return self

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        for reader in self.readers:
            reader.__exit__(exc_type, exc_val, exc_tb)
        if self.bar is not None:
            self.bar.goto(self.bar.max)
            self.bar.finish()

    def __next__(self):
        # Key is readers' index
        reading_from = [
            key
            for key, value in self._readers_data.items()
            if (value["haste"] < self.haste_limit and not value["finished"])
        ]

        # Read batches from not hasting readers
        batches_data = {}
        for key in reading_from:
            # If batch is present, drop "density" col and set "chr" and "position"
            # sorted for further sorted merge
            try:
                batch = next(self._readers_data[key]["iterator"])
                batches_data[key] = (
                    batch.data.set_sorted(["chr", "position"])
                    .drop("density")
                    .with_columns(pl.lit([np.uint8(key)]).alias("group_idx"))
                )
                # Upd metadata
                self._readers_data[key]["read_rows"] += len(batch)
                self._readers_data[key] |= dict(
                    chr=batch.data[-1]["chr"].to_list()[0],
                    pos=batch.data[-1]["position"].to_list()[0],
                )
            # Else mark reader as finished
            except StopIteration:
                self._readers_data[key]["finished"] = True

        if not batches_data and len(self._unfinished) == 0:
            raise StopIteration
        elif batches_data:
            self.haste_limit = sum(len(batch) for batch in batches_data.values()) // 3

        # We assume that input file is sorted in some way
        # So we gather chromosomes order in the input files to understand which is last
        if self._unfinished is not None:
            batches_data[-1] = self._unfinished

        pack_seen_chroms = []
        for key in batches_data:
            batch_chrs = batches_data[key]["chr"].unique(maintain_order=True).to_list()
            [
                pack_seen_chroms.append(chrom)
                for chrom in batch_chrs
                if chrom not in pack_seen_chroms
            ]
        [
            self._seen_chroms.append(chrom)
            for chrom in pack_seen_chroms
            if chrom not in self._seen_chroms
        ]

        # Merge read batches and unfinished data with each other and then group
        merged = []
        for chrom in pack_seen_chroms:
            # Retrieve first batch (order doesn't matter) with which we will merge
            chr_merged = batches_data[list(batches_data.keys())[0]].filter(chr=chrom)
            # Get keys of other batches
            other = [key for key in batches_data if key != list(batches_data.keys())[0]]
            for key in other:
                chr_merged = chr_merged.merge_sorted(
                    batches_data[key].filter(chr=chrom), key="position"
                )
            merged.append(chr_merged)
        merged = pl.concat(merged).set_sorted("chr", "position")

        # Group merged rows by chromosome and position and check indexes that
        # have merged
        grouped = (
            merged.lazy()
            .group_by(["chr", "position"], maintain_order=True)
            .agg(
                [
                    pl.first("strand", "context", "trinuc"),
                    pl.sum("count_m", "count_total"),
                    pl.col("group_idx").explode(),
                ]
            )
            .with_columns(pl.col("group_idx").list.len().alias("group_count"))
        )

        # Finished rows are those which have grouped among all readers
        min_chr_idx = min(
            self._seen_chroms.index(reader_data["chr"])
            for reader_data in self._readers_data.values()
        )
        min_position = min(
            reader_data["pos"]
            for reader_data in self._readers_data.values()
            if reader_data["chr"] == self._seen_chroms[min_chr_idx]
        )

        # Finished if all readers have grouped or there is no chance to group because
        # position is already skipped
        marked = (
            grouped.with_columns(
                [
                    pl.col("chr")
                    .replace(self._seen_chroms, list(range(len(self._seen_chroms))))
                    .cast(pl.Int8)
                    .alias("chr_idx"),
                    pl.lit(min_position).alias("min_pos"),
                ]
            )
            .with_columns(
                pl.when(
                    (pl.col("group_count") == len(self._readers_data))
                    | (
                        (pl.col("chr_idx") < min_chr_idx)
                        | (pl.col("chr_idx") == min_chr_idx)
                        & (pl.col("position") < pl.col("min_pos"))
                    )
                )
                .then(pl.lit(True))
                .otherwise(pl.lit(False))
                .alias("finished")
            )
            .collect()
        )

        self._unfinished = marked.filter(finished=False).select(merged.columns)

        hasting_stats = defaultdict(int)
        if len(self._unfinished) > 0:
            group_idx_stats = self._unfinished.select(
                pl.col("group_idx").list.to_struct()
            ).unnest("group_idx")
            for col in group_idx_stats.columns:
                hasting_stats[group_idx_stats[col].drop_nulls().item(0)] += (
                    group_idx_stats[col].count()
                )

        for key in self._readers_data:
            self._readers_data[key]["haste"] = hasting_stats[key]

        # Update bar
        if reading_from:
            self.bar.next(sum(self.readers[idx].batch_size for idx in reading_from))

        out = marked.filter(finished=True)
        return UniversalBatch(self._convert_to_full(out), out.to_arrow())

    @staticmethod
    def _convert_to_full(df: pl.DataFrame):
        return df.with_columns(
            (pl.col("count_m") / pl.col("count_total")).alias("density")
        ).select(UniversalBatch.schema.polars.names())

    @property
    def full_size(self):
        """

        Returns
        -------
        int
            Total size of readers' files in bytes.
        """
        return sum(map(lambda reader: reader.file_size, self.readers))


class UniversalWriter:
    """
    Class for writing reports in specific methylation report format.

    Parameters
    ----------
    file
        Path where the file will be written.
    report_type
        Type of the methylation report. Possible types: "bismark", "cgmap", "bedgraph",
        "coverage", "binom"
    """

    def __init__(
        self,
        file: str | Path,
        report_type: ReportSchema,
    ):
        file = Path(file).expanduser().absolute()
        self.file = file

        if report_type not in ReportSchema.__members__:
            raise KeyError(report_type)
        self.report_type = report_type

        self.memory_pool = pa.system_memory_pool()
        self.writer = None

    def __enter__(self):
        self.open()
        return self

    def close(self):
        """
        This method should be called after writing all data, when writer is called
        without `with` context manager.
        """
        self.writer.close()
        self.memory_pool.release_unused()

    def open(self):
        """
        This method should be called before writing data, when writer is called without
        `with` context manager.
        """
        write_options = pcsv.WriteOptions(
            delimiter="\t", include_header=False, quoting_style="none"
        )

        self.writer = pcsv.CSVWriter(
            self.file,
            self.report_type.arrow,
            write_options=write_options,
            memory_pool=self.memory_pool,
        )

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()

    def write(self, universal_batch: UniversalBatch):
        """
        Method for writing batch to the report file.
        """
        if universal_batch is None:
            return

        if self.writer is None:
            self.__enter__()

        if self.report_type == ReportSchema.BISMARK:
            fmt_df = universal_batch.cast(ReportSchema.BISMARK)
        elif self.report_type == ReportSchema.CGMAP:
            fmt_df = universal_batch.cast(ReportSchema.CGMAP)
        elif self.report_type == ReportSchema.BEDGRAPH:
            fmt_df = universal_batch.cast(ReportSchema.BEDGRAPH)
        elif self.report_type == ReportSchema.COVERAGE:
            fmt_df = universal_batch.cast(ReportSchema.COVERAGE)
        else:
            raise KeyError(f"{self.report_type} not supported")

        self.writer.write(fmt_df.to_arrow().cast(self.report_type.arrow))
